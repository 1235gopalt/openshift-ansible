
- name: Check for appropriate Docker versions
  hosts: oo_masters_to_config:oo_nodes_to_config:oo_etcd_to_config
  roles:
  - openshift_facts
  tasks:
  - set_fact:
      repoquery_cmd: "{{ 'dnf repoquery --latest-limit 1 -d 0' if ansible_pkg_mgr == 'dnf' else 'repoquery' }}"

  - fail:
      msg: Cannot upgrade Docker on Atomic hosts
    when: openshift.common.is_atomic | bool

  - name: Determine available Docker version
    script: ../../../../common/openshift-cluster/upgrades/files/rpm_versions.sh docker
    register: g_docker_version_result

  - name: Check if Docker is installed
    command: rpm -q docker
    register: pkg_check
    failed_when: pkg_check.rc > 1
    changed_when: no

  - name: Get current version of Docker
    command: "{{ repoquery_cmd }} --installed --qf '%{version}' docker"
    register: curr_docker_version
    changed_when: false

  - name: Get latest available version of Docker
    command: >
      {{ repoquery_cmd }} --qf '%{version}' "docker"
    register: avail_docker_version
    failed_when: false
    changed_when: false

  - fail:
      msg: This playbook requires access to Docker 1.10 or later
    # Disable the 1.10 requirement if the user set a specific Docker version
    when: avail_docker_version.stdout | version_compare('1.10','<') and docker_version is not defined

  - name: Flag for upgrade if Docker version does not equal latest
    set_fact:
      docker_upgrade: true
    when: docker_version is not defined and pkg_check.rc == 0 and curr_docker_version.stdout | version_compare(avail_docker_version.stdout,'<')

  - name: Flag for upgrade if Docker version does not equal requested version
    set_fact:
      docker_upgrade: true
    when: docker_version is defined and pkg_check.rc == 0 and curr_docker_version.stdout | version_compare(docker_version,'<')


# If a node fails, halt everything, the admin will need to clean up and we
# don't want to carry on, potentially taking out every node. The playbook can safely be re-run
# and will not take any action on a node already running the requested docker version.
- name: Evacuate and upgrade nodes
  hosts: oo_masters_to_config:oo_nodes_to_config:oo_etcd_to_config
  serial: 1
  any_errors_fatal: true
  tasks:
  - name: Prepare for Node evacuation
    command: >
      {{ openshift.common.admin_binary }} manage-node {{ openshift.common.hostname | lower }} --schedulable=false
    delegate_to: "{{ groups.oo_first_master.0 }}"
    when: docker_upgrade is defined and docker_upgrade | bool and inventory_hostname in groups.oo_nodes_to_config

  - name: Evacuate Node for Kubelet upgrade
    command: >
      {{ openshift.common.admin_binary }} manage-node {{ openshift.common.hostname | lower }} --evacuate --force
    delegate_to: "{{ groups.oo_first_master.0 }}"
    when: docker_upgrade is defined and docker_upgrade | bool and inventory_hostname in groups.oo_nodes_to_config

  - name: Stop containerized services
    service: name={{ item }} state=stopped
    with_items:
      - "{{ openshift.common.service_type }}-master"
      - "{{ openshift.common.service_type }}-master-api"
      - "{{ openshift.common.service_type }}-master-controllers"
      - "{{ openshift.common.service_type }}-node"
      - etcd_container
      - openvswitch
    failed_when: false
    when: docker_upgrade is defined and docker_upgrade | bool and openshift.common.is_containerized | bool

  - name: Remove all containers and images
    script: files/nuke_images.sh docker
    register: nuke_images_result
    when: docker_upgrade is defined and docker_upgrade | bool

  # TODO: should we use the docker role to actually do the upgrade?
  - name: Upgrade to specified Docker version
    action: "{{ ansible_pkg_mgr }} name=docker{{ '-' + docker_version }} state=present"
    register: docker_upgrade_result
    when: docker_upgrade is defined and docker_upgrade | bool and docker_version is defined

  - name: Upgrade to latest Docker version
    action: "{{ ansible_pkg_mgr }} name=docker state=latest"
    register: docker_upgrade_result
    when: docker_upgrade is defined and docker_upgrade | bool and docker_version is not defined

  - name: Restart containerized services
    service: name={{ item }} state=started
    with_items:
      - etcd_container
      - openvswitch
      - "{{ openshift.common.service_type }}-master"
      - "{{ openshift.common.service_type }}-master-api"
      - "{{ openshift.common.service_type }}-master-controllers"
      - "{{ openshift.common.service_type }}-node"
    failed_when: false
    when: docker_upgrade is defined and docker_upgrade | bool and openshift.common.is_containerized | bool

  - name: Wait for master API to come back online
    become: no
    local_action:
      module: wait_for
        host="{{ inventory_hostname }}"
        state=started
        delay=10
        port="{{ openshift.master.api_port }}"
    when: docker_upgrade is defined and docker_upgrade | bool and inventory_hostname in groups.oo_masters_to_config

  - name: Set node schedulability
    command: >
      {{ openshift.common.admin_binary }} manage-node {{ openshift.common.hostname | lower }} --schedulable=true
    delegate_to: "{{ groups.oo_first_master.0 }}"
    when: openshift.node.schedulable | bool
    when: docker_upgrade is defined and docker_upgrade | bool and inventory_hostname in groups.oo_nodes_to_config and openshift.node.schedulable | bool

